\subsection{Алгоритм обратного распространения ошибки}
\indent \indent Определим ошибку в нейроне с номером $j$ в слое $l$ следующим образом:

$$
\delta_j^l = \frac{\delta J}{\delta z_j^l}.
\eqno(16)
$$

Тогда частная производная целевой функции по сумматорной будет иметь вид: 
$$\mathlarger{\frac{\delta J}{\delta z_j^l}} = (\sigma(z_j) - \sigma^2(z_j)) \cdot (\sigma(z_j) - y_j).
\eqno(17)
$$.

Будем обозначать $\nabla_a J$ ~--- градиент целевой функции по активациям входного слоя, а $\odot$ ~--- произведение Адамара({поэлементное умножение векторов).

Следовательно из формул (16) и (17) можно получить вектор ошибок выходного слоя:

$$
\delta_L = \nabla_a J \odot \sigma'(z^L).
\eqno(18)
$$

Суть алгоритма обратного распространения ошибки - зная ошибку на слое с номером $l$, узнать ошибку на $l-1$ слое.
Рассмотрим любой нейрон с номером $j$ в слое $l$. Этот нейрон связан со всеми нейронами из слоя $l+1$. Соответственно, если увеличить значение его активационной функции, то это изменение домноженное на соответствующие веса пойдет в нейроны следующего слоя. Допустим значение активационной функции было $a_j^l$ и стало $a_j^l + \Delta$. Можно узнать как изменится значение сумматорной функции каждого из нейронов следующего слоя. До изменения активация нейрона равнялась $a_j^l w_{jk}^l + C$, где $C$ - константа, а после добавления $\Delta$ получаем, что активация изменилась на
 $$
\Delta \cdot w_{jk}^l .
\eqno(19)
$$
Известно, что изменение функции равно произведению градиента на разницу вектора аргументов $\Delta J \approx \nabla J \cdot \Delta x$. Если вектор изменения входных активаций слоя $l+1$ равен $(\Delta \cdot w_{1k}^{l+1}, \ldots, \Delta \cdot w_{n_{l+1}k}^{l+1})$ и $\Delta$ мало, то тогда получаем:

$$
\Delta J = \Delta \cdot (W^{l+1})^T_k \cdot \delta^{l+1},
$$

где $(W^{l+1})^T_k$ ~--- $k$-й столбец матрицы весов между слоем $l$ и $l+1$.

Выразим ошибку $\delta^l$ через ошибку $\delta^{l+1}$. Из формулы (18) известно, что 
$$
\delta_l = \nabla_{a^l} J \odot \sigma'(z^l).
\eqno(20)
$$

Осталось выразить $\nabla_{a^l} J$ через $\delta^{l+1}$.
$\nabla_{a^l} J$ состоит из частной производной $\mathlarger{\frac{\partial J}{\partial a_j^l}}$.

$$
\Delta J = \Delta \cdot (W^{l+1})^T_k \cdot \delta^{l+1},
\eqno(21)
$$
где $(W^{l+1})^T_k$ ~--- это $k$-й столбец матрицы $W^{l+1}$.
Следовательно, $\mathlarger{\frac{\partial J}{\partial a_i^l} = (W^{l+1})^T_k \cdot \delta^{l+1}}$.

\noindent Из формулы (21) получаем, что $\nabla_{a^l} J = (W^{l+1})^T_k \cdot \delta^{l+1}$. Итого: 
$$
\delta^l = ((W^{l+1})^T \cdot \delta^{l+1}) \odot \sigma '(z^l).
\eqno(22)
$$

Сумматорную функцию нейрона $j$ в слое $l$ можно выразить через активационную функцию нейронов из слоя $l-1$.$z_j^l = W_j^l \cdot a_{l-1} + b_j^l$

Тогда частняа производная целевой функции по весу $\frac{\partial J}{\partial W_{jk}^l}$ будет равна $a_k^{l-1} \cdot \delta_j^l$, а по смещению: $\frac{\partial J}{\partial w_{jk}^l} = \delta_j^l$.

Все нужные формулы получены. Теперь можно выделить 4 основных, которые будут участвовать в алгоритме обратного распространения ошибки:
\begin{enumerate}
  \item[1)] $\delta_L = \nabla_{a} J \odot \sigma'(z^L)$
  \item[2)] $\delta^l = ((W^{l+1})^T \cdot \delta^{l+1}) \odot \sigma '(z^l)$
  \item[3)]  $\frac{\partial J}{\partial w_{jk}^l} = \delta_j^l$
  \item[4)]  $\frac{\partial J}{\partial W_{jk}^l} = a_k^{l-1} \cdot \delta_j^l$
\end{enumerate}

Первая формула из списка помогает получить вектор ошибок выходного слоя, зная градиент целевой функции по активациям нейронов выходного слоя $\nabla_a J$. 
По второй формуле можно вычислить ошибки в слое $l-1$. Это делает возможным осуществить шаг алгоритма с дальнего слоя к ближнему. 

Третья и четвертая формула позволяют вычислить нужные производные(по смещению и по любому из весов).

\subsubsection{Шаги алгоритма обратного распространения ошибки}

\indent \indent Используя формулы из предыдущего пункта, получаем алгоритм обратного распространения ошибки.

\begin{itemize}
  \item Выбрать $m$ примеров, на основании которых будет производиться изменение весов;
  \item осуществить прямое распространение активации для каждого примера;
  \item использовать первую формулу, чтобы посчитать ошибки нейронов в выходном слое по каждому примеру;
  \item использовать вторую формулу, для нахождения ошибок нейронов во всех слоях по каждому примеру;
  \item использовать третью и четвертую формулу, чтобы получить градиенты по параметрам и смещениям;
  \item обновить параметры;
  \item проверить критерии остановки алгоритма.
\end{itemize}

\subsubsection{Нормализация}

\indent \indent Для правильной работы алгоритма обратного распространения ошибки важно, чтобы веса были разные. Действительно, если все веса одинаковые, то все нейроны из первого скрытого слоя получат одинаковую активацию. Потому что каждый нейрон связан с каждым входом скрытого слоя. Соответственно, никакой разницы между нейронами первого скрытого слоя не существует. 
Эту проблему можно решить, инициализируя веса случайными значениями. Например, нормальный шум или равномерный шум.

Большие векторы весов, также как и больше векторы входных активаций могут привести к тому, что производная будет близка к нулю. Соответственно, можно получить состояние \textit{ступора сети}. Если активации имеют разный масштаб, то и градиенты будут иметь разный масштаб. Поэтому очень важно правильно нормализовать входы.