\subsection{Алгоритм обратного распространения ошибки}
Определим ошибку в нейроне с номером $j$ в слое $l$ следующим образом:

$$
\delta_j^l = \frac{\delta J}{\delta z_j^l}
\eqno(16)
$$

Тогда частная производная целевой функции по сумматорной будет иметь вид: 
$$\mathlarger{\frac{\delta J}{\delta z_j^l}} = (\sigma(z_j) - \sigma^2(z_j)) \cdot (\sigma(z_j) - y_j)
\eqno(17)
$$.

Будем обозначать $\nabla_a J$ - градиент целевой функции по активациям входного слоя, а $\odot$ - произведение Адамара(\textit{поэлементное умножение векторов}).

Следовательно из формул (16) и (17) можно получить вектор ошибок выходного слоя:

$$
\delta_L = \nabla_a J \odot \sigma'(z^L)
\eqno(18)
$$

Суть алгоритма обратного распространения ошибки - зная ошибку на слое с номером $l$, узнать ошибку на $l-1$ слое.
Рассмотрим любой нейрон с номером $j$ в слое $l$. Этот нейрон связан со всеми нейронами из слоя $l+1$. Соответственно, если увеличить значение его активационной функции, то это изменение домноженное на соответствующие веса пойдет в нейроны следующего слоя. Допустим значение активационной функции было $a_j^l$ и стало $a_j^l + \Delta$. Можно узнать как изменится значение сумматорной функции каждого из нейронов следующего слоя. До изменения активация нейрона равнялась $a_j^l w_{jk}^l + C$, где $C$ - константа, а после добавления $\Delta$ получаем, что активация изменилась на
 $$
\Delta \cdot w_{jk}^l
\eqno(19)
$$.
Известно, что изменение функции равно произведению градиента на разницу вектора аргументов $\Delta J \approx \nabla J \cdot \Delta x$. Если вектор изменения входных активаций слоя $l+1$ равен $(\Delta \cdot w_{1k}^{l+1}, \ldots, \Delta \cdot w_{n_{l+1}k}^{l+1})$ и $\Delta$ мало, то тогда получаем:

$$
\Delta J = \Delta \cdot (W^{l+1})^T_k \cdot \delta^{l+1}
$$

Где $(W^{l+1})^T_k$ - $k$-й столбец матрицы весов между слоем $l$ и $l+1$.

Выразим ошибку $\delta^l$ через ошибку $\delta^{l+1}$. Из формулы (18) известно, что 
$$
\delta_L = \nabla_{a^l} J \odot \sigma'(z^l)
\eqno(20)
$$

Осталось выразить $\nabla_{a^l} J$ через $\delta^{l+1}$.
$\nabla_{a^l} J$ состоит из частной производной $\mathlarger{\frac{\partial J}{\partial a_j^l}}$.

$$
\Delta J = \Delta \cdot (W^{l+1})^T_k \cdot \delta^{l+1}
\eqno(21)
$$
Где $(W^{l+1})^T_k$ - это $k$-й столбец матрицы $W^{l+1}$.
Следовательно, $\mathlarger{\frac{\partial J}{\partial a_i^l} = (W^{l+1})^T_k \cdot \delta^{l+1}}$.

\noindent Из формулы (21) получаем, что $\nabla_{a^l} J = (W^{l+1})^T_k \cdot \delta^{l+1}$. Итого: 
$$
\delta^l = (W^{l+1})^T \cdot \delta^{l+1} \odot \sigma '(z^l)
\eqno(22)
$$