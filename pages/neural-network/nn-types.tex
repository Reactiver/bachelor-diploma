\section{Виды нейронных сетей}.

 \textbf{Нейронные сети прямого распространения 
(feed forward neural networks, FF} или \textbf{FFNN)} и 
\textbf{перцептроны (perceptrons)} очень прямолинейны, они передают информацию от входа к выходу. Это означает, что в сети нет петель - информация всегда передается, никогда не возвращается. Если в сети были циклы, то получилась бы ситуация, когда вход в функцию зависит от выхода. Нейроны одного слоя не связаны между собой, а соседние слои обычно полностью связаны. Самая простая нейронная сеть имеет две входных клетки и одну выходную, и может использоваться в качестве модели логических вентилей. FFNN обычно обучается по методу обратного распространения ошибки, в котором сеть получает множества входных и выходных данных. Этот процесс называется обучением с учителем, и он отличается от обучения без учителя тем, что во втором случае множество выходных данных сеть составляет самостоятельно. Практически такие сети используются редко, но их часто комбинируют с другими типами для получения новых. \\

Однако есть и другие модели искусственных нейронных сетей, в которых возможны петли обратной связи. Эти модели называются \textbf{рекуррентными нейронными сетями}. Идея в этих моделях состоит в том, чтобы иметь нейроны, которые срабатывают в течение некоторого ограниченного периода времени, прежде чем станут спокойными. Это срабатывание может стимулировать другие нейроны, которые могут срабатывать немного позже, также в течение ограниченного периода времени. Это вызывает запуск еще большего количества нейронов, и поэтому со временем получается каскад запуска нейронов. Циклы не вызывают проблем в такой модели, поскольку выход нейрона влияет только на его вход через некоторое время, а не мгновенно. \\ \\

Рекуррентные нейронные сети были менее влиятельными, чем сети с прямой связью, отчасти потому, что алгоритмы обучения для рекуррентных сетей (по крайней мере на сегодняшний день) менее эффективны. Но повторяющиеся сети все еще чрезвычайно интересны. По духу они гораздо ближе к тому, как работает наш мозг, чем к сетям прямой связи. И возможно, что повторяющиеся сети могут решить важные проблемы, которые могут быть решены с большим трудом только через сети прямой связи. \\ \\

\textbf{Нейронная сеть Хопфилда (Hopfield network, HN)} — это полносвязная нейронная сеть с симметричной матрицей связей. Во время получения входных данных каждый узел является входом, в процессе обучения он становится скрытым, а затем становится выходом. Сеть обучается так: значения нейронов устанавливаются в соответствии с желаемым шаблоном, после чего вычисляются веса, которые в дальнейшем не меняются. После того, как сеть обучилась на одном или нескольких шаблонах, она всегда будет сводиться к одному из них (но не всегда — к желаемому). Она стабилизируется в зависимости от общей “энергии” и “температуры” сети. У каждого нейрона есть свой порог активации, зависящий от температуры, при прохождении которого нейрон принимает одно из двух значений (обычно -1 или 1, иногда 0 или 1).  Такая сеть часто называется сетью с ассоциативной памятью; как человек, видя половину таблицы, может представить вторую половину таблицы, так и эта сеть, получая таблицу, наполовину зашумленную, восстанавливает её до полной. \\ \\

\textbf{Цепи Маркова (Markov chains, MC или discrete time Markov Chains, DTMC)} — это предшественники машин Больцмана (BM) и сетей Хопфилда (HN). Их смысл можно объяснить так: каковы мои шансы попасть в один из следующих узлов, если я нахожусь в данном? Каждое следующее состояние зависит только от предыдущего. Хотя на самом деле цепи Маркова не являются НС, они весьма похожи. Также цепи Маркова не обязательно полносвязны. \\ \\

\textbf{Свёрточные нейронные сети (convolutional neural networks, CNN) и глубинные свёрточные нейронные сети (deep convolutional neural networks, DCNN)} - тут надо их описать
