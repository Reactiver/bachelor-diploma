\section{Виды нейронных сетей}.

 \textbf{Нейронные сети прямого распространения 
(feed forward neural networks, FF} или \textbf{FFNN)} и 
\textbf{перцептроны (perceptrons)} имеют прямолинейное распространение, они передают информацию от входа к выходу. Это означает, что в сети нет петель ~--- информация всегда передается, никогда не возвращается. Нейроны одного слоя не связаны между собой, а соседние слои обычно полностью связаны. FFNN обычно обучается по методу обратного распространения ошибки, в котором сеть получает множества входных и выходных данных.

Однако есть и другие модели искусственных нейронных сетей, в которых возможны петли обратной связи. Эти модели называются \textbf{рекуррентными нейронными сетями}. Рекуррентная нейронная сеть (RNN) - это класс искусственной нейронной сети, где соединения между узлами образуют ориентированный граф вдоль временной последовательности. В отличие от нейронных сетей с прямой связью, RNN могут использовать свое внутреннее состояние (память) для обработки последовательностей входных данных. Это делает их применимыми к таким задачам, как несегментированное распознавание рукописного ввода с подключением или распознавание речи.

\textbf{Нейронная сеть Хопфилда (Hopfield network, HN)} является формой рекуррентной искусственной нейронной сети, популяризированной Дж. Хопфилдом в 1982 году. Сети Хопфилда служат в качестве адресно-ориентированных («ассоциативных») систем памяти с двоичными пороговыми узлами. Они гарантированно сходятся к локальному минимуму и, следовательно, могут сходиться к ложному шаблону (неправильный локальный минимум), а не к сохраненному шаблону (ожидаемый локальный минимум). Сети Хопфилда также предоставляют модель для понимания человеческой памяти.

\textbf{Цепи Маркова (Markov chains, MC или discrete time Markov Chains, DTMC)} — это стохастическая модель, описывающая последовательность возможных событий, в которой вероятность каждого события зависит только от состояния, достигнутого в предыдущем событии.

\textbf{Свёрточные нейронные сети (convolutional neural networks, CNN)} являются регуляризованными версиями многослойных персептронов. Многослойные персептроны обычно относятся к полностью соединенным сетям, то есть каждый нейрон в одном слое связан со всеми нейронами в следующем слое. «Полная связность» этих сетей делает их склонными к перегрузке данных. Типичные способы регуляризации включают добавление некоторой формы измерения весов к функции потерь. Однако CNN используют другой подход к регуляризации: они используют преимущества иерархического шаблона в данных и собирают более сложные шаблоны, используя меньшие и более простые шаблоны. Таким образом, по шкале связанности и сложности CNN находятся на более низком уровне.
